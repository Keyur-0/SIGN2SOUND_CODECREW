# SIGN2SOUND â€“ CodeCrew

## ğŸ”Š Bridging Communication Through AI

SIGN2SOUND is a multimodal accessibility system designed to reduce communication barriers between **speech users and sign language users**.  
The project focuses on **real-time Speech-to-Text and Sign-to-Text translation**, with a scalable architecture that supports **Sign-to-Speech** as a future extension.

This repository contains the **Phase-2 implementation** for the SIGN2SOUND challenge.

---

## ğŸ¯ Problem Statement
Communication between hearing individuals and sign language users remains a major accessibility challenge. Existing solutions are often expensive, language-specific, or not real-time.

SIGN2SOUND aims to:
- Enable **real-time interaction**
- Use **AI-based recognition**
- Remain **lightweight and scalable**
- Support **bidirectional communication**

---

## âœ… Core Features

### ğŸŸ¢ Speech â†’ Text (Implemented)
- Real-time speech recognition using **Vosk**
- Offline processing (no internet required)
- Converts spoken language into readable text
- Live speech captions are rendered in a real-time OpenCV GUI with start/stop controls.

#### Audio Input Note
By default, the Speech â†’ Text module captures audio from a microphone device.
For online meeting transcription (e.g., Zoom, Google Meet), the system can
capture speaker audio using system-level loopback or monitor devices
(PipeWire / PulseAudio).

This enables transcription of meeting audio without modifying the core
speech recognition pipeline.

### ğŸŸ¢ Sign â†’ Text (Implemented)
- Real-time sign language recognition using **skeletal keypoints**
- Uses **MediaPipe** for landmark extraction
- Deep learning model (PyTorch LSTM-based sequence model)
- Converts signs into text output

### ğŸ”µ Sign â†’ Speech (Scalable / Optional)
- Converts recognized sign text into audio
- Implemented as a **modular extension**
- Can be enabled without retraining the model

---

## ğŸ§  System Architecture
```

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Speech User â”€â”€Micâ”€â”€â–¶â”‚   VOSK     â”‚â”€â”€â–¶ Text Output
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sign User â”€â”€Cameraâ”€â”€â–¶ MediaPipe â”€â–¶ LSTM Model â”€â–¶ Text Output
                                      â”‚
                                      â–¼
                               (Optional)
                                  TTS
```
---

## ğŸ“Š Dataset Information
This project uses an **IEEE DataPort dataset** as required by the challenge:

- **Indian Sign Language Skeletal-point NumPy Array (MediaPipe)**
  - Contains pre-extracted skeletal keypoints
  - Suitable for time-series modeling
  - Enables fast training and real-time inference

ğŸ“Œ Due to dataset licensing, **raw data is not included** in this repository.  
### Dataset sources and usage instructions are documented in:
```bash
 data/README.md
```
---

## ğŸ—ï¸ Project Structure (Simplified)
```
SIGN2SOUND_CodeCrew/
â”‚
â”œâ”€â”€ checkpoints/                     # Trained model weights
â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”œâ”€â”€ final_model.pth
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/                            # Dataset documentation (IEEE compliant)
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ statistics.txt
â”‚
â”œâ”€â”€ preprocessing/                   # Data preprocessing pipeline
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ extract_features.py
â”‚   â”œâ”€â”€ augmentation.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ features/                        # Feature extraction modules
â”‚   â”œâ”€â”€ hand_landmarks.py
â”‚   â”œâ”€â”€ pose_estimation.py
â”‚   â”œâ”€â”€ facial_features.py
â”‚   â”œâ”€â”€ feature_utils.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ models/                          # Model architecture definitions
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ custom_layers.py
â”‚   â”œâ”€â”€ loss.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ training/                        # Training & evaluation pipeline
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ label_utils.py
â”‚   â”œâ”€â”€ callbacks.py
â”‚   â”œâ”€â”€ hyperparams.yaml
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ inference/                       # Real-time inference & demo
â”‚   â”œâ”€â”€ infer.py
â”‚   â”œâ”€â”€ opencv_gui.py                # Main Phase-2 demo
â”‚   â”œâ”€â”€ realtime_demo.py
â”‚   â”œâ”€â”€ tts.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ speech_to_text/                  # Offline Speech â†’ Text module
â”‚   â”œâ”€â”€ vosk_stt.py
â”‚   â”œâ”€â”€ models/                      # Vosk language models
â”‚   â””â”€â”€ __pycache__/
â”‚
â”œâ”€â”€ notebooks/                       # Experiments & analysis
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_experiment.ipynb
â”‚   â”œâ”€â”€ 03_results_visualization.ipynb
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ results/                         # Evaluation outputs
â”‚   â”œâ”€â”€ accuracy_curves.png
â”‚   â”œâ”€â”€ loss_curves.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ metrics.json
â”‚   â”œâ”€â”€ per_class_performance.csv
â”‚   â”œâ”€â”€ training_log.txt
â”‚   â”œâ”€â”€ plot_curves.py
â”‚   â””â”€â”€ sample_outputs/
â”‚       â”œâ”€â”€ sample_1.png
â”‚       â”œâ”€â”€ sample_2.png
â”‚       â””â”€â”€ predictions.txt
â”‚
â”œâ”€â”€ docs/                            # Documentation & reports
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ system_pipeline.png
â”‚   â”œâ”€â”€ dataset_preprocessing.md
â”‚   â”œâ”€â”€ training_details.md
â”‚   â””â”€â”€ technical_report.pdf
â”‚
â”œâ”€â”€ tests/                           # Unit tests
â”‚   â”œâ”€â”€ test_inference.py
â”‚   â””â”€â”€ test_model.py
â”‚
â”œâ”€â”€ README.md                        # Main project documentation
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```
---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Environment Setup
```bash
python -m venv venv311
source venv311/bin/activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run Speech â†’ Text (Vosk)
```bash
python speech_to_text/vosk_stt.py
```

### 3ï¸âƒ£ Run Sign â†’ Text (Webcam Demo)
```bash
python -m inference.opencv_gui
```

## ğŸ¥ Demo

A real-time multimodal demo is provided showcasing both **Sign â†’ Text** and **Speech â†’ Text** capabilities of the SIGN2SOUND system through a unified OpenCV-based interface.

### The demo demonstrates:

* **Live webcam capture**
* **Hand landmark extraction** using MediaPipe
* **Real-time Sign â†’ Text recognition** using an LSTM-based model
* **Temporal prediction smoothing** to stabilize sign outputs
* **Offline Speech â†’ Text transcription** using Vosk
* **Live visual feedback** including:

  * Current sign prediction
  * Stable sign output
  * Speech transcription
  * System status indicators

The Sign â†’ Text and Speech â†’ Text pipelines operate independently but are visualized together to demonstrate **bidirectional accessibility**.

### Running the demo:

```bash
python -m inference.opencv_gui
```

### Sample Outputs:

Screenshots and example predictions from the demo are available in:

```bash
results/sample_outputs/
```

---

## ğŸ“ˆ Evaluation & Results
### The system is evaluated using standard classification metrics:
    - Accuracy
    - Precision
    - Recall
    - F1-score
    - Confusion Matrix
    - Training & Validation loss curves

### All evaluation outputs are stored in:
```bash
    results/
```

---
## ğŸ§ª Testing

Basic unit tests are provided to verify model forward passes and inference
utilities. These tests are lightweight and designed to ensure functional
correctness without requiring external hardware or datasets.

Run tests using:
```bash
python -m tests.test_model
python -m tests.test_inference
```

---

## ğŸ“„ Documentation

### Comprehensive documentation is available in the docs/ directory:
- architecture_diagram.png â€“ Model architecture overview
- system_pipeline.png â€“ End-to-end system flow
- technical_report.pdf â€“ Detailed technical explanation
- dataset_preprocessing.md â€“ Data preparation details
- training_details.md â€“ Training configuration & procedure

---

## ğŸ”® Future Enhancements

- Full Sign â†’ Speech integration
- Sentence-level and continuous sign recognition
- Support for additional sign languages
- Bidirectional conversational interface
- Deployment on web and mobile platforms

---

## ğŸ‘¥ Team
 CodeCrew
 SIGN2SOUND Challenge â€“ Phase 2

---

## ğŸ“œ License
This project is released under the MIT License.

---

## ğŸ™ Acknowledgements

IEEE DataPort for providing datasets
MediaPipe for skeletal landmark extraction
Vosk for offline speech recognition

---
