# SIGN2SOUND â€“ CodeCrew

## ğŸ”Š Bridging Communication Through AI

SIGN2SOUND is a multimodal accessibility system designed to reduce communication barriers between **speech users and sign language users**.  
The project focuses on **real-time Speech-to-Text and Sign-to-Text translation**, with a scalable architecture that supports **Sign-to-Speech** as a future extension.

This repository contains the **Phase-2 implementation** for the SIGN2SOUND challenge.

---

## ğŸ¯ Problem Statement
Communication between hearing individuals and sign language users remains a major accessibility challenge. Existing solutions are often expensive, language-specific, or not real-time.

SIGN2SOUND aims to:
- Enable **real-time interaction**
- Use **AI-based recognition**
- Remain **lightweight and scalable**
- Support **bidirectional communication**

---

## âœ… Core Features

### ğŸŸ¢ Speech â†’ Text (Implemented)
- Real-time speech recognition using **Vosk**
- Offline processing (no internet required)
- Converts spoken language into readable text

#### Audio Input Note
By default, the Speech â†’ Text module captures audio from a microphone device.
For online meeting transcription (e.g., Zoom, Google Meet), the system can
capture speaker audio using system-level loopback or monitor devices
(PipeWire / PulseAudio).

This enables transcription of meeting audio without modifying the core
speech recognition pipeline.

### ğŸŸ¢ Sign â†’ Text (Implemented)
- Real-time sign language recognition using **skeletal keypoints**
- Uses **MediaPipe** for landmark extraction
- Deep learning model (PyTorch LSTM/Bi-LSTM)
- Converts signs into text output

### ğŸ”µ Sign â†’ Speech (Scalable / Optional)
- Converts recognized sign text into audio
- Implemented as a **modular extension**
- Can be enabled without retraining the model

---

## ğŸ§  System Architecture
```bash

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Speech User â”€â”€Micâ”€â”€â–¶â”‚   VOSK     â”‚â”€â”€â–¶ Text Output
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sign User â”€â”€Cameraâ”€â”€â–¶ MediaPipe â”€â–¶ LSTM Model â”€â–¶ Text Output
                                      â”‚
                                      â–¼
                               (Optional)
                                  TTS
```
---

## ğŸ“Š Dataset Information
This project uses an **IEEE DataPort dataset** as required by the challenge:

- **Indian Sign Language Skeletal-point NumPy Array (MediaPipe)**
  - Contains pre-extracted skeletal keypoints
  - Suitable for time-series modeling
  - Enables fast training and real-time inference

ğŸ“Œ Due to dataset licensing, **raw data is not included** in this repository.  
### Dataset sources and usage instructions are documented in:
```bash
 data/README.md
```
---

## ğŸ—ï¸ Project Structure (Simplified)
```bash
SIGN2SOUND_CodeCrew/
â”œâ”€â”€ data/ # Dataset documentation (IEEE compliant)
â”œâ”€â”€ preprocessing/ # Data preprocessing pipeline
â”œâ”€â”€ features/ # MediaPipe landmark extraction
â”œâ”€â”€ models/ # PyTorch model architecture
â”œâ”€â”€ training/ # Training & evaluation scripts
â”œâ”€â”€ inference/ # Real-time inference & demo
â”œâ”€â”€ results/ # Metrics, graphs, outputs
â”œâ”€â”€ checkpoints/ # Trained model weights
â”œâ”€â”€ docs/ # Diagrams & technical report
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Environment Setup
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run Speech â†’ Text (Vosk)
```bash
python speech_to_text/vosk_stt.py
```

### 3ï¸âƒ£ Run Sign â†’ Text (Webcam Demo)
```bash
python inference/realtime_demo.py
```

---

## ğŸ“ˆ Evaluation & Results
### The system is evaluated using standard classification metrics:
    - Accuracy
    - Precision
    - Recall
    - F1-score
    - Confusion Matrix
    - Training & Validation loss curves

### All evaluation outputs are stored in:
```bash
    results/
```

---

## ğŸ“„ Documentation

### Comprehensive documentation is available in the docs/ directory:
- architecture_diagram.png â€“ Model architecture overview
- system_pipeline.png â€“ End-to-end system flow
- technical_report.pdf â€“ Detailed technical explanation
- dataset_preprocessing.md â€“ Data preparation details
- training_details.md â€“ Training configuration & procedure

---

## ğŸ”® Future Enhancements

- Full Sign â†’ Speech integration
- Sentence-level and continuous sign recognition
- Support for additional sign languages
- Bidirectional conversational interface
- Deployment on web and mobile platforms

---

## ğŸ‘¥ Team
 CodeCrew
 SIGN2SOUND Challenge â€“ Phase 2

---

## ğŸ“œ License
This project is released under the MIT License (or applicable license).

---

## ğŸ™ Acknowledgements

IEEE DataPort for providing datasets
MediaPipe for skeletal landmark extraction
Vosk for offline speech recognition

---