# ================================
# SIGN2SOUND – Training Hyperparameters
# ================================

# Dataset
dataset:
  name: "Indian Sign Language Skeletal-point NumPy Array (MediaPipe)"
  input_features: 63          # 21 hand landmarks × (x, y, z)
  sequence_length: 30
  num_classes: 25
  total_samples: 3000
  train_split: 0.8
  val_split: 0.2
  shuffle: true

# Model Architecture
model:
  type: "LSTM"
  framework: "PyTorch"
  input_size: 63
  hidden_size: 256
  num_layers: 2
  dropout: 0.3
  output_layer: "Fully Connected"
  activation: "Softmax (via CrossEntropyLoss)"

# Training Configuration
training:
  batch_size: 32
  epochs: 40
  optimizer: "Adam"
  learning_rate: 0.001
  loss_function: "CrossEntropyLoss"
  weight_decay: 0.0
  gradient_clipping: false

# Hardware
environment:
  device: "CPU"
  platform: "Local Machine / Google Colab Pro (optional)"
  framework_version:
    pytorch: ">=2.0"
    python: "3.11"

# Evaluation
evaluation:
  metrics:
    - accuracy
    - loss
    - confusion_matrix
  final_train_accuracy: 0.768
  final_val_accuracy: 0.758

# Checkpoints
checkpoint:
  save_path: "checkpoints/sign_lstm.pth"
  save_best_only: true

# Notes
notes:
  - "Model trained on skeletal hand landmarks extracted using MediaPipe"
  - "Temporal modeling handled via LSTM over 30-frame sliding windows"
  - "Prediction smoothing applied during real-time inference"
